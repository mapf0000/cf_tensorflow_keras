{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Collaborative Filtering in Tensorflow using a Latent Factor Model</h1>\n",
    "\n",
    "<img src=\"img/user_item.svg\">\n",
    "<img src=\"img/latent_factors.svg\">\n",
    "<img src=\"img/predict.svg\">\n",
    "\n",
    "Prediction:\n",
    "$$\n",
    "r' _ { u i } = \\mu + b _ { u } + b _ { i } + p _ { u } q _ { i }\n",
    "$$\n",
    "\n",
    "Loss Function:\n",
    "$$\n",
    "\\min _ { p _ { * } , q _ { * } , b _ { * } } \\sum _ { ( u , i ) \\in \\mathcal { K } } \\left( r _ { u i } - \\mu - b _ { u } - b _ { i } - p _ { u } ^ { T } q _ { i } \\right) ^ { 2 } + \\lambda \\left( \\| p _ { u } \\| ^ { 2 } + \\| q _ { i } \\| ^ { 2 } + b _ { u } ^ { 2 } + b _ { i } ^ { 2 } \\right)\n",
    "$$\n",
    "\n",
    "<br><br><br><br>\n",
    "Further Reading:<br>\n",
    "<a href=\"http://sifter.org/simon/journal/20061211.html\">Basic Latent Factor Model, original source</a><br>\n",
    "<a href=\"http://www.cs.rochester.edu/twiki/pub/Main/HarpSeminar/Factorization_Meets_the_Neighborhood-_a_Multifaceted_Collaborative_Filtering_Model.pdf\">Advanced Latent Factor Models</a><br>\n",
    "<a href=\"http://www.cs.ubbcluj.ro/~gabis/DocDiplome/SistemeDeRecomandare/Recommender_systems_handbook.pdf\">The Recommender Systems Handbook</a><br>\n",
    "<a href=\"https://arxiv.org/pdf/1708.05031.pdf\">Neural Collaborative Filtering</a><br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import models.svd as svd\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.initializers import glorot_normal\n",
    "from tensorflow.python.keras.regularizers import l2\n",
    "from tensorflow.python.keras.layers import Input, Embedding, concatenate, Dense, Flatten, Dropout\n",
    "\n",
    "from collections import Counter\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movie_lens_1m():\n",
    "    movie_lens_1m = pd.read_csv(\"datasets/ml-1m/ratings.dat\", sep='::', header=None, engine='python')\n",
    "        \n",
    "    x, y = movie_lens_1m.iloc[:, :2].values, movie_lens_1m.iloc[:, 2].values \n",
    "    user_dict = dict(enumerate(np.unique(x[:, 0])))\n",
    "    item_dict = dict(enumerate(np.unique(x[:, 1])))\n",
    "    x[:, 0] = [{value:key for key,value in user_dict.items()}[u] for u in x[:, 0]] # index users from 0 to num_users - 1\n",
    "    x[:, 1] = [{value:key for key,value in item_dict.items()}[i] for i in x[:, 1]] # index items from 0 to num_items - 1\n",
    "\n",
    "    return x, y, user_dict, item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movie_lens_100k_norm():\n",
    "    movie_lens_100k = pd.read_csv(\"datasets/ml-100k/u.data\", sep='\\t', header=None, engine='python')\n",
    "        \n",
    "    x, y = movie_lens_100k.iloc[:, :2].values, movie_lens_100k.iloc[:, 2].values \n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    y = min_max_scaler.fit_transform(y.reshape(-1, 1))\n",
    "    \n",
    "    user_dict = dict(enumerate(np.unique(x[:, 0])))\n",
    "    item_dict = dict(enumerate(np.unique(x[:, 1])))\n",
    "    x[:, 0] = [{value:key for key,value in user_dict.items()}[u] for u in x[:, 0]] # index users from 0 to num_users - 1\n",
    "    x[:, 1] = [{value:key for key,value in item_dict.items()}[i] for i in x[:, 1]] # index items from 0 to num_items - 1\n",
    "\n",
    "    return x, y, user_dict, item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movie_lens_100k():\n",
    "    movie_lens_100k = pd.read_csv(\"datasets/ml-100k/u.data\", sep='\\t', header=None, engine='python')\n",
    "        \n",
    "    x, y = movie_lens_100k.iloc[:, :2].values, movie_lens_100k.iloc[:, 2].values \n",
    "    user_dict = dict(enumerate(np.unique(x[:, 0])))\n",
    "    item_dict = dict(enumerate(np.unique(x[:, 1])))\n",
    "    x[:, 0] = [{value:key for key,value in user_dict.items()}[u] for u in x[:, 0]] # index users from 0 to num_users - 1\n",
    "    x[:, 1] = [{value:key for key,value in item_dict.items()}[i] for i in x[:, 1]] # index items from 0 to num_items - 1\n",
    "\n",
    "    return x, y, user_dict, item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y, user_dict, item_dict = load_movie_lens_100k()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable user_embeddings already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\D072013\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\D072013\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\D072013\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-1f7036e3647c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msvd_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msvd_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\tensorflow_workspace\\tensorflow_recommender\\models\\svd.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, x, y, num_factors, optimizer, x_valid, y_valid, batch_size, epochs, reg_p_u, reg_b_u, reg_q_i, reg_b_i)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# build model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         training_objective, prediction = self.build_model(\n\u001b[1;32m--> 150\u001b[1;33m             users, items, ratings)\n\u001b[0m\u001b[0;32m    151\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_objective\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'optimizer'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\tensorflow_workspace\\tensorflow_recommender\\models\\svd.py\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(self, users, items, ratings)\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_users\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_factors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglorot_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m             regularizer=tf.keras.regularizers.l2(self.reg_p_u))\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         user_bias = tf.get_variable(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1260\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1262\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1263\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1264\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1095\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1097\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    433\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    402\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 743\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    744\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable user_embeddings already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"C:\\Users\\D072013\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n  File \"C:\\Users\\D072013\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\D072013\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "svd_model = svd.SVD(sess)\n",
    "svd_model.train(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "parameters = dict()\n",
    "parameters[\"num_users\"] = np.unique(X[:, 0]).size\n",
    "parameters[\"num_items\"] = np.unique(X[:, 1]).size\n",
    "parameters[\"num_factors\"] = 100\n",
    "\n",
    "# regularization hyperparameters\n",
    "hyperparameters = dict()\n",
    "hyperparameters[\"reg_b_u\"] = 0.0001\n",
    "hyperparameters[\"reg_b_i\"] = 0.0001\n",
    "hyperparameters[\"reg_p_u\"] = 0.005\n",
    "hyperparameters[\"reg_q_i\"] = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_constants(mu):\n",
    "    \"\"\"\n",
    "    Creates and returns mu constant,\n",
    "    which is defined as mean over all ratings.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('constants'):\n",
    "        _mu = tf.constant(mu, shape=[], dtype=tf.float32)\n",
    "    \n",
    "    return _mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_user_variables(users, parameters, hyperparameters):\n",
    "    \"\"\"\n",
    "    Creates latent user features_fac and user bias.\n",
    "    Returns look-up OPs.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('users'):\n",
    "        user_embeddings = tf.get_variable(\n",
    "            name='embedding',\n",
    "            shape=[parameters[\"num_users\"], parameters[\"num_factors\"]],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(hyperparameters[\"reg_p_u\"]))\n",
    "\n",
    "        user_bias = tf.get_variable(\n",
    "            name='bias',\n",
    "            shape=[parameters[\"num_users\"], ],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(hyperparameters[\"reg_b_u\"]))\n",
    "\n",
    "        p_u = tf.nn.embedding_lookup(\n",
    "            user_embeddings,\n",
    "            users,\n",
    "            name='p_u')\n",
    "\n",
    "        b_u = tf.nn.embedding_lookup(\n",
    "            user_bias,\n",
    "            users,\n",
    "            name='b_u')\n",
    "    \n",
    "    return p_u, b_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_item_variables(items, parameters, hyperparameters):\n",
    "    \"\"\"\n",
    "    Creates latent item features and item bias.\n",
    "    Returns look-up OPs.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('items'):\n",
    "        item_embeddings = tf.get_variable(\n",
    "            name='embedding',\n",
    "            shape=[parameters[\"num_items\"], parameters[\"num_factors\"]],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(hyperparameters[\"reg_q_i\"]))\n",
    "\n",
    "        item_bias = tf.get_variable(\n",
    "            name='bias',\n",
    "            shape=[parameters[\"num_items\"], ],\n",
    "            initializer=tf.contrib.layers.xavier_initializer(),\n",
    "            regularizer=tf.contrib.layers.l2_regularizer(hyperparameters[\"reg_b_i\"]))\n",
    "\n",
    "        q_i = tf.nn.embedding_lookup(\n",
    "            item_embeddings,\n",
    "            items,\n",
    "            name='q_i')\n",
    "\n",
    "        b_i = tf.nn.embedding_lookup(\n",
    "            item_bias,\n",
    "            items,\n",
    "            name='b_i')\n",
    "\n",
    "    return q_i, b_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_prediction(mu, b_u, b_i, p_u, q_i):\n",
    "    \"\"\" \n",
    "    Returns the prediction which is definded as:\n",
    "    r_hat = \\mu + b_u + b_i + p_u * q_i\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('prediction'):\n",
    "        pred = tf.reduce_sum(\n",
    "            tf.multiply(p_u, q_i),\n",
    "            axis=1)\n",
    "\n",
    "        pred = tf.add_n([b_u, b_i, pred])\n",
    "\n",
    "        pred = tf.add(pred, mu, name='pred')\n",
    "\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_loss(pred, ratings):\n",
    "    \"\"\"\n",
    "    Returns the L2 loss.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('loss'):\n",
    "        loss = tf.nn.l2_loss(tf.subtract(ratings, pred), name='loss')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_metrics(pred, ratings):\n",
    "    \"\"\" \n",
    "    Returns evaluation Metrics and update OPs.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('metrics'):\n",
    "        mae, mae_update_op = tf.metrics.mean_absolute_error(ratings, pred, name =\"mae\")\n",
    "    \n",
    "        rmse, rmse_update_op = tf.metrics.root_mean_squared_error(tf.cast(ratings, tf.float32), tf.cast(pred, tf.float32), name =\"rmse\")\n",
    "    \n",
    "    return mae, mae_update_op, rmse, rmse_update_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_optimizer(loss):\n",
    "    \"\"\"\n",
    "    Returns the optimizer.\n",
    "    The objective function is defined as the sum of\n",
    "    loss and regularizers' losses.\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('optimizer'):\n",
    "        objective = tf.add(\n",
    "            loss,\n",
    "            tf.add_n(tf.get_collection(\n",
    "                tf.GraphKeys.REGULARIZATION_LOSSES)),\n",
    "            name='objective')\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer().minimize(objective, name='optimizer')\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.01).minimize(objective, name='optimizer',)\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(users, items, ratings, mu, parameters, hyperparameters):\n",
    "    _mu = create_constants(mu)\n",
    "\n",
    "    p_u, b_u = create_user_variables(users, parameters, hyperparameters)\n",
    "    q_i, b_i = create_item_variables(items, parameters, hyperparameters)\n",
    "\n",
    "    pred = create_prediction(_mu, b_u, b_i, p_u, q_i)\n",
    "\n",
    "    loss = create_loss(pred, ratings)\n",
    "\n",
    "    optimizer = create_optimizer(loss)\n",
    "    \n",
    "    return optimizer, loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(x, y, parameters, hyperparameters, epochs=30, batch_size=64, validation_data=None):\n",
    "\n",
    "    if x.shape[0] != y.shape[0] or x.shape[1] != 2:\n",
    "        raise ValueError('The shape of x should be (samples, 2) and '\n",
    "                             'the shape of y should be (samples, 1).')\n",
    "\n",
    "    # create datasets\n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        (x[:, 0].astype(np.int32), \n",
    "         x[:, 1].astype(np.int32), \n",
    "         y.astype(np.float32))).batch(batch_size)\n",
    "    \n",
    "    if validation_data is not None:\n",
    "        valid_x, valid_y = validation_data\n",
    "        validation_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "            (valid_x[:, 0].astype(np.int32), \n",
    "             valid_x[:, 1].astype(np.int32), \n",
    "             valid_y.astype(np.float32))).batch(batch_size)\n",
    "\n",
    "    # create dataset iterator\n",
    "    iter = tf.data.Iterator.from_structure(training_dataset.output_types,\n",
    "                                           training_dataset.output_shapes)\n",
    "\n",
    "    users, items, ratings = iter.get_next()\n",
    "    \n",
    "    training_init_op = iter.make_initializer(training_dataset)  \n",
    "    if validation_data is not None:\n",
    "        validation_init_op = iter.make_initializer(validation_dataset)\n",
    "    \n",
    "    # build model\n",
    "    optimizer, loss, pred = build_graph(users, items, ratings, np.mean(y), parameters, hyperparameters)\n",
    "    \n",
    "    if validation_data is not None:\n",
    "        mae, mae_update_op, rmse, rmse_update_op = create_metrics(pred, ratings)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        print('Training...')\n",
    "        for e in range(0, epochs):\n",
    "            print('Epoch {}/{}'.format(e + 1, epochs))\n",
    "            \n",
    "            # training\n",
    "            sess.run(training_init_op)\n",
    "            mean_loss = 0.0\n",
    "            counter = 0\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    _, loss_value = sess.run([optimizer, loss])\n",
    "                    mean_loss += loss_value/batch_size\n",
    "                    counter += 1\n",
    "                except tf.errors.OutOfRangeError: \n",
    "                    # after last batch\n",
    "                    break\n",
    "            \n",
    "            print(\"Train loss: {:.4f}\".format(mean_loss/counter ))\n",
    "            \n",
    "            # validation\n",
    "            if validation_data is not None:\n",
    "                sess.run(tf.local_variables_initializer())\n",
    "                sess.run(validation_init_op)\n",
    "                mean_loss = 0.0\n",
    "                counter = 0\n",
    "            \n",
    "                while True:\n",
    "                    try:\n",
    "                        loss_value, _, _ = sess.run([loss, mae_update_op, rmse_update_op])\n",
    "                        mean_loss += loss_value/batch_size\n",
    "                        counter += 1\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        # after last batch\n",
    "                        break\n",
    "            \n",
    "                mae_val, rmse_val = sess.run([mae, rmse])\n",
    "                print(\"Validation loss: {:.4f} | MAE: {:.4f} | RMSE: {:.4f}\".format(mean_loss/counter, mae_val, rmse_val))\n",
    "            \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, \"logdir/model.ckpt\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_neural_cf_model(num_users, num_items, num_factors):\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    user_embeddings = Embedding(input_dim = num_users, output_dim = num_factors, name = 'user_embeddings', \n",
    "                                   embeddings_initializer='glorot_normal', embeddings_regularizer=l2(0.01),  input_length=1)\n",
    "    item_embeddings = Embedding(input_dim = num_items, output_dim = num_factors, name = 'item_embeddings', \n",
    "                                   embeddings_initializer='glorot_normal', embeddings_regularizer=l2(0.01), input_length=1)\n",
    "    \n",
    "    concat_input = concatenate([Flatten()(user_embeddings(user_input)), Flatten()(item_embeddings(item_input))])\n",
    "    \n",
    "    layer1 = Dense(128, activation='relu')(concat_input)\n",
    "    drop1  = Dropout(0.5)(layer1)\n",
    "    layer2 = Dense(128, activation='relu')(drop1)\n",
    "    drop2  = Dropout(0.5)(layer2)\n",
    "    layer3 = Dense(64, activation='relu')(drop2)\n",
    "    drop3  = Dropout(0.5)(layer3)\n",
    "    layer4 = Dense(64, activation='relu')(drop3)\n",
    "    drop4  = Dropout(0.5)(layer4)\n",
    "    pred_layer = Dense(1, name='prediction')(drop4)\n",
    "    \n",
    "    model = Model(inputs=[user_input, item_input], outputs=pred_layer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 23s 252us/step - loss: 1.9705 - mean_absolute_error: 1.0009 - val_loss: 1.3244 - val_mean_absolute_error: 0.8290\n",
      "\n",
      "Epoch 2/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 22s 246us/step - loss: 1.4792 - mean_absolute_error: 0.8860 - val_loss: 1.2370 - val_mean_absolute_error: 0.7956\n",
      "\n",
      "Epoch 3/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 21s 235us/step - loss: 1.3522 - mean_absolute_error: 0.8415 - val_loss: 1.2045 - val_mean_absolute_error: 0.7847\n",
      "\n",
      "Epoch 4/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 22s 247us/step - loss: 1.2657 - mean_absolute_error: 0.8127 - val_loss: 1.1774 - val_mean_absolute_error: 0.7936\n",
      "\n",
      "Epoch 5/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 21s 239us/step - loss: 1.2161 - mean_absolute_error: 0.7976 - val_loss: 1.1602 - val_mean_absolute_error: 0.7793\n",
      "\n",
      "Epoch 6/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 22s 243us/step - loss: 1.1924 - mean_absolute_error: 0.7936 - val_loss: 1.1593 - val_mean_absolute_error: 0.7811\n",
      "\n",
      "Epoch 7/10\n",
      "90000/90000 [==============================]90000/90000 [==============================] - 22s 244us/step - loss: 1.1783 - mean_absolute_error: 0.7879 - val_loss: 1.1613 - val_mean_absolute_error: 0.7878\n",
      "\n",
      "Epoch 8/10\n",
      "15520/90000 [====>.........................]15520/90000 [====>.........................] - ETA: 18s - loss: 1.1611 - mean_absolute_error: 0.7785"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-7f0eaeeb0f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m model.fit([X_train[:, 0], X_train[:, 1]], Y_train, epochs=10, batch_size=32,\n\u001b[1;32m---> 10\u001b[1;33m           validation_data=([X_valid[:, 0], X_valid[:, 1]], Y_valid))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1677\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1678\u001b[1;33m         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1221\u001b[0m           \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\_impl\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2551\u001b[0m     \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2552\u001b[0m     updated = session.run(\n\u001b[1;32m-> 2553\u001b[1;33m         fetches=fetches, feed_dict=feed_dict, **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2554\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_neural_cf_model(parameters[\"num_users\"], parameters[\"num_items\"], parameters[\"num_factors\"])\n",
    "\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "              loss='mean_squared_error',\n",
    "              metrics=['mae'])\n",
    "\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "\n",
    "model.fit([X_train[:, 0], X_train[:, 1]], Y_train, epochs=10, batch_size=32,\n",
    "          validation_data=([X_valid[:, 0], X_valid[:, 1]], Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/30\n",
      "Train loss: 2.5426\n",
      "Validation loss: 2.5175 | MAE: 0.2340 | RMSE: 0.2793\n",
      "Epoch 2/30\n",
      "Train loss: 2.5250\n",
      "Validation loss: 2.5253 | MAE: 0.2341 | RMSE: 0.2798\n",
      "Epoch 3/30\n",
      "Train loss: 2.5142\n",
      "Validation loss: 2.5309 | MAE: 0.2340 | RMSE: 0.2801\n",
      "Epoch 4/30\n",
      "Train loss: 2.5083\n",
      "Validation loss: 2.5353 | MAE: 0.2338 | RMSE: 0.2801\n",
      "Epoch 5/30\n",
      "Train loss: 2.5060\n",
      "Validation loss: 2.5393 | MAE: 0.2339 | RMSE: 0.2803\n",
      "Epoch 6/30\n",
      "Train loss: 2.5050\n",
      "Validation loss: 2.5417 | MAE: 0.2339 | RMSE: 0.2804\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-86084ab4d366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-ada6cc760af8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(x, y, parameters, hyperparameters, epochs, batch_size, validation_data)\u001b[0m\n\u001b[0;32m     48\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m                     \u001b[0mmean_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "train(X_train, Y_train, parameters, hyperparameters, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = [\"movie id\", \"movie title\", \"release date\", \"video release date\",\n",
    "              \"IMDb URL\", \"Genre unknown\", \"Action\", \"Adventure\", \"Animation\",\n",
    "              \"Children's\", \"Comedy\", \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\",\n",
    "              \"Film-Noir\", \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\",\n",
    "              \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "items_info = pd.read_csv(\"datasets/ml-100k/u.item\", sep='|', names = headers, index_col=0, engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "items_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[379]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[228]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[226]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[229]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[221]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[227]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[449]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[180]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[171]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#items_info.loc[item_dict[49]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
